# NN_transplant
## _A precious lesson to learn for a neural network freshman (<- yeah ...... it's me~)._

## Description:
#### This 3-layer neural network was referred from CS231n course of Stanford university. And the backbone of these codes was referred from mattmazur/simple_neural_network. I intended to realized the word2vec network based on the structure of mattmazur's code. Let's see how far I could go!

#### While...since the model was designed receiving only one input a time, it performed like a linear Softmax classifier even though the structure as well as the gradient descending algorithm were designed for realizing a 3-layer neural network (Sigmoid function applied for hidden layer, and Softmax function applied for output layer).

## Structure:  
##### Input layer: 2 neurons representing X and Y axis
##### Hidden layer: 100 neurons
##### Output layer: 3 neurons representing 3 classess

## Brief conclusion and Future:
##### For a quick understand to the structure of neural network, I've made my hand dirty and have this 3-layer network born.
##### The materials and implementations will be stored in this respiratory. I'll be appreciated for any form of feedback!
##### https://github.com/geminiTW1990s/DL-RL-NLP-materials-digestion
##### In the future, I'll go further into field of deep learning networks. Let's see how far I could go!
